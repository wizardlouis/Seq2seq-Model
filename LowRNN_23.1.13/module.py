# -*- codeing = utf-8 -*-
# @time:2022/6/28 下午8:32
# Author:Xuewen Shen
# @File:module.py
# @Software:PyCharm
import torch
import torch.nn as nn
import numpy as np
import time
import os

#Identify training process and pipeline setup
freq_report = 5
freq_save = 10

def MSELoss(Prediction,Target,Mask):
    #Input/Target/Mask of the same shape: (Batch_Size,time_steps,out_dim)
    loss_by_trial=(Mask*(Target-Prediction)).pow(2).sum(dim=-1).sum(dim=-1)
    count_timesteps=Mask.sum(dim=-1).sum(dim=-1)
    return (loss_by_trial/count_timesteps).mean()

#training 1 epoch for reparameterlized model
def train_reparameter(model,optimizer,scheduler,Input,Target,Mask,device='cpu',reg_p=False,clip_gradient=False,**kwargs):
    #input trials are generated by specific data_generating functions
    optimizer.zero_grad()
    if 'g_rec' in kwargs:
        g_rec=kwargs['g_rec']
    else:
        g_rec=model.P['g_rec']
    Output=model(Input,g_rec=g_rec,device=device)[:,1:]
    loss_position=MSELoss(Output,Target,Mask)
    if reg_p:
        loss_reg=sum(model.reg_loss_p(device=device))
    else:
        loss_reg=sum(model.reg_loss(device=device))
    loss = loss_position + model.P['w_reg'] * loss_reg
    loss.backward()
    if clip_gradient:
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)
    optimizer.step()
    if scheduler is not None:
        scheduler.step(loss)
    return np.array([L.item() for L in [loss,loss_position,loss_reg]])

#training process for reparameterlized model
def Mixed_Epoch_train_reparamter(Batch_generator,model,optimizer,scheduler,device='cpu',reg_p=False,clip_gradient=False,**kwargs):
    n_converge = 0
    start0 = time.time()
    save_count=0
    savepath=model.P['savepath']
    if not os.path.exists(savepath):
        os.makedirs(savepath)

    f = open(savepath + '//report.txt', 'a')
    f.write('Report of simulation:\n')
    f.close()

    # save initial model
    torch.save(model, savepath + '//model_0.pth')
    save_count += 1

    for epoch in range(model.P['n_epochs']):
        start=time.time()
        Epoch_loss=np.zeros((3,),dtype=float)
        model.set_sampling(device=device)
        for batch in range(model.P['N_Batch']):
            Input,Target,Mask=Batch_generator(model.P,device=device)
            Epoch_loss+=train_reparameter(model,optimizer,scheduler,Input,Target,Mask,device=device,reg_p=reg_p,clip_gradient=clip_gradient,**kwargs)

        if epoch % freq_report == freq_report - 1:
            end = time.time()
            f = open(savepath + '//report.txt', 'a')
            f.write(
                '\nEpoch {}:\nTotal Loss = {}\nPositional Loss={}\nRegularization Loss = {}'
                    .format(str(epoch + 1), str(Epoch_loss[0]), str(Epoch_loss[1]), str(Epoch_loss[2])
                            )
            )

            f.write('\nThis Epoch takes:{} seconds.\nThe whole process takes:{} seconds'.format(str(end - start),
                                                                                                str(end - start0)))
            f.close()
        # save model
        if epoch % freq_save == freq_save - 1:
            torch.save(model, savepath + '//model_' + str(save_count) + '.pth')
            save_count += 1
        if not os.path.exists(savepath + '//loss.npz'):
            np.savez(savepath + '//loss.npz', name=np.array(['Loss', 'Loss_p', 'Loss_r']),
                     data=np.array([Epoch_loss]))
        else:
            Lossfile = np.load(savepath + '//loss.npz')
            np.savez(savepath + '//loss.npz', name=Lossfile['name'],
                     data=np.concatenate((Lossfile['data'], np.array([Epoch_loss])), axis=0))
        if Epoch_loss[0]>0.01:
            n_converge=0
        else:
            n_converge+=1
        if n_converge==3:
            break

    end0 = time.time()
    f=open(savepath+'//report.txt','a')
    f.write('\nThe whole Training Process finished in {} seconds!!!'.format(str(end0-start0)))
    return True

def train(model,optimizer,scheduler,Input,Target,Mask,device='cpu',clip_gradient=False,**kwargs):
    #Input trials are generated by specific data_generator functions
    optimizer.zero_grad()
    if 'g_rec' in kwargs:
        g_rec = kwargs['g_rec']
    else:
        g_rec = model.P['g_rec']
    Output = model(Input, g_rec=g_rec, device=device)[:, 1:]
    loss_position = MSELoss(Output, Target, Mask)
    loss_reg=sum(model.reg_loss())
    loss = loss_position + model.P['w_reg'] * loss_reg
    loss.backward()
    if clip_gradient:
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)
    optimizer.step()
    if scheduler is not None:
        scheduler.step(loss)
    return np.array([L.item() for L in [loss, loss_position, loss_reg]])

def Mixed_Epoch_train(Batch_generator,model,optimizer,scheduler,device='cpu',clip_gradient=False,**kwargs):
    n_converge=0
    start0 = time.time()
    save_count=0
    savepath=model.P['savepath']
    if not os.path.exists(savepath):
        os.makedirs(savepath)

    f = open(savepath + '//report.txt', 'a')
    f.write('Report of simulation:\n')
    f.close()

    # save initial model
    torch.save(model, savepath + '//model_0.pth')
    save_count += 1

    for epoch in range(model.P['n_epochs']):
        start=time.time()
        Epoch_loss=np.zeros((3,),dtype=float)
        for batch in range(model.P['N_Batch']):
            Input,Target,Mask=Batch_generator(model.P,device=device)
            Epoch_loss+=train(model,optimizer,scheduler,Input,Target,Mask,device=device,clip_gradient=clip_gradient,**kwargs)

        if epoch % freq_report == freq_report - 1:
            end = time.time()
            f = open(savepath + '//report.txt', 'a')
            f.write(
                '\nEpoch {}:\nTotal Loss = {}\nPositional Loss={}\nRegularization Loss = {}'
                    .format(str(epoch + 1), str(Epoch_loss[0]), str(Epoch_loss[1]), str(Epoch_loss[2])
                            )
            )

            f.write('\nThis Epoch takes:{} seconds.\nThe whole process takes:{} seconds'.format(str(end - start),
                                                                                                str(end - start0)))
            f.close()
        # save model
        if epoch % freq_save == freq_save - 1:
            torch.save(model, savepath + '//model_' + str(save_count) + '.pth')
            save_count += 1
        if not os.path.exists(savepath + '//loss.npz'):
            np.savez(savepath + '//loss.npz', name=np.array(['Loss', 'Loss_p', 'Loss_r']),
                     data=np.array([Epoch_loss]))
        else:
            Lossfile = np.load(savepath + '//loss.npz')
            np.savez(savepath + '//loss.npz', name=Lossfile['name'],
                     data=np.concatenate((Lossfile['data'], np.array([Epoch_loss])), axis=0))
        if Epoch_loss[0]>0.01:
            n_converge=0
        else:
            n_converge+=1
        if n_converge==3:
            break

    end0 = time.time()
    f=open(savepath+'//report.txt','a')
    f.write('\nThe whole Training Process finished in {} seconds!!!'.format(str(end0-start0)))
    return True